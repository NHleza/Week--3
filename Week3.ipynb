{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHTz6mDuHLBwwEgIIrALuy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NHleza/Week--3/blob/main/Week3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AI Tools Assignment: Mastering the AI Toolkit 🛠️🧠\n",
        "# Members: [Hleza Nqobile]\n",
        "\n",
        "# ===========================\n",
        "# PART 1: Image Classification with TensorFlow (Fashion MNIST)\n",
        "# ===========================\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Fashion MNIST dataset from TensorFlow Datasets\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'fashion_mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "# Normalize images: uint8 [0,255] -> float32 [0.0,1.0]\n",
        "def normalize_img(image, label):\n",
        "    return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "# Prepare training and test datasets\n",
        "ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_train = ds_train.cache().shuffle(ds_info.splits['train'].num_examples).batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_test = ds_test.batch(128).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Build a simple CNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28,28,1)),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Dropout(0.25),  # Regularization\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "history = model.fit(ds_train, epochs=10, validation_data=ds_test)\n",
        "\n",
        "# Plot training and validation accuracy and loss\n",
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fashion_mnist_training_plot.png\")  # Save for report\n",
        "plt.show()\n",
        "\n",
        "# Evaluate model on test dataset\n",
        "test_images = []\n",
        "test_labels = []\n",
        "for images, labels in tfds.as_numpy(ds_test):\n",
        "    test_images.append(images)\n",
        "    test_labels.append(labels)\n",
        "test_images = np.vstack(test_images)\n",
        "test_labels = np.hstack(test_labels)\n",
        "\n",
        "predictions = model.predict(test_images)\n",
        "pred_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(test_labels, pred_labels, target_names=ds_info.features['label'].names))\n",
        "\n",
        "# Confusion matrix heatmap\n",
        "cm = confusion_matrix(test_labels, pred_labels)\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=ds_info.features['label'].names, yticklabels=ds_info.features['label'].names, cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig(\"fashion_mnist_confusion_matrix.png\")  # Save for report\n",
        "plt.show()\n",
        "\n",
        "# ===========================\n",
        "# PART 2: Named Entity Recognition (NER) with spaCy\n",
        "# ===========================\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Load pre-trained English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Real-world example text for NER\n",
        "text = (\n",
        "    \"Apple is looking at buying U.K. startup for $1 billion. \"\n",
        "    \"Elon Musk founded SpaceX in 2002 and Tesla in 2003.\"\n",
        ")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Named Entities in the text:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} - {ent.label_}\")\n",
        "\n",
        "# Render and save NER visualization as SVG\n",
        "svg = displacy.render(doc, style=\"ent\", jupyter=False)\n",
        "with open(\"ner_visualization.svg\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(svg)\n",
        "\n",
        "# ===========================\n",
        "# PART 3: Ethical Reflection (to include in your report)\n",
        "# ===========================\n",
        "\n",
        "\"\"\"\n",
        "Ethical Reflection:\n",
        "\n",
        "- Bias & Fairness: Fashion MNIST is a balanced dataset, but real-world datasets may have biases that must be identified and mitigated.\n",
        "- Privacy: No personal or sensitive data is used here. For real applications, ensure data privacy and consent.\n",
        "- Transparency: Model architecture, hyperparameters, and training details are documented for reproducibility.\n",
        "- Optimization: Regularization (dropout, L2) was applied to prevent overfitting. Further hyperparameter tuning can improve performance.\n",
        "- Responsible Use: AI outputs, especially in sensitive domains, should be validated by domain experts before deployment.\n",
        "\"\"\"\n",
        "\n",
        "# ===========================\n",
        "# END OF NOTEBOOK\n",
        "# ===========================\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WPIGxZuc3KRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eaClxYVu3Fzj"
      },
      "outputs": [],
      "source": []
    }
  ]
}